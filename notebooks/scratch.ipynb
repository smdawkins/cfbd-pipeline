{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3232f8e5-32bb-4976-a925-fb0cd22f13f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def build_session(api_key: str) -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\"Authorization\": f\"Bearer {api_key}\"})\n",
    "    s.timeout = 60\n",
    "    return s\n",
    "\n",
    "def fetch_json(session: requests.Session, base_url: str, path: str):\n",
    "    r = session.get(f\"{base_url}{path}\")\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data if isinstance(data, list) else [data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8bc6d8d-84d0-42b5-a28a-96ce50a3b432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json, datetime\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "#AUDIT_SCHEMA = \"raw string,  _season int, _week int, _source string\"\n",
    "AUDIT_SCHEMA = \"raw string, _ingest_ts string, _season int, _week int, _source string\"\n",
    "\n",
    "def write_bronze_raw(spark, records, endpoint_name: str, bronze_schema: str, season: int, week: int):\n",
    "    ingest_ts = datetime.datetime.utcnow().isoformat()\n",
    "    rows = [(json.dumps(rec),  ingest_ts, int(season), int(week), endpoint_name) for rec in records]\n",
    "    df = spark.createDataFrame(rows, AUDIT_SCHEMA)\n",
    "    target = f\"{bronze_schema}.{endpoint_name}_raw\"\n",
    "    if not spark.catalog.tableExists(target):\n",
    "        (df.write\n",
    "           .format(\"delta\")\n",
    "           .mode(\"overwrite\")\n",
    "           .partitionBy(\"_season\",\"_week\")\n",
    "           .saveAsTable(target))\n",
    "        return\n",
    "    (df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"replaceWhere\", f\"_season = {season} AND _week = {week}\")\n",
    "       .saveAsTable(target))\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "663bc30d-84dd-4ed1-96ed-dd58406ade8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, ArrayType\n",
    "\n",
    "def infer_schema_from_sample(spark, raw_tbl: str, season: int, week: int):\n",
    "    sample = (spark.table(raw_tbl)\n",
    "                  .where((F.col(\"_season\")==season) & (F.col(\"_week\")==week))\n",
    "                  .select(\"raw\").limit(1).collect())\n",
    "    if not sample: return None\n",
    "    sample_json = sample[0][\"raw\"]\n",
    "    return (spark.range(1)\n",
    "                 .select(F.schema_of_json(F.lit(sample_json)).alias(\"s\"))\n",
    "                 .collect()[0][\"s\"])\n",
    "\n",
    "def parse_raw_to_cols(spark, raw_tbl: str, season: int, week: int, schema: StructType):\n",
    "    return (spark.table(raw_tbl)\n",
    "            .where((F.col(\"_season\")==season) & (F.col(\"_week\")==week))\n",
    "            .withColumn(\"obj\", F.from_json(F.col(\"raw\"), schema))\n",
    "            .select(\"obj.*\",\"_ingest_ts\",\"_season\",\"_week\",\"_source\"))\n",
    "\n",
    "def flatten_structs_until_done(df, sep=\"_\"):\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        new_cols = []\n",
    "        for f in df.schema.fields:\n",
    "            if isinstance(f.dataType, StructType):\n",
    "                for sub in f.dataType.fields:\n",
    "                    new_cols.append(F.col(f\"{f.name}.{sub.name}\").alias(f\"{f.name}{sep}{sub.name}\"))\n",
    "                changed = True\n",
    "            else:\n",
    "                new_cols.append(F.col(f.name))\n",
    "        df = df.select(*new_cols)\n",
    "    return df\n",
    "\n",
    "def flatten_and_explode_all(df, sep=\"_\"):\n",
    "    \"\"\"\n",
    "    Recursively flatten structs, then explode one array at a time,\n",
    "    until only primitive columns remain.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # flatten one struct\n",
    "        flattened = False\n",
    "        for f in df.schema.fields:\n",
    "            if isinstance(f.dataType, StructType):\n",
    "                keep = [c for c in df.columns if c != f.name]\n",
    "                exp  = [F.col(f\"{f.name}.{s.name}\").alias(f\"{f.name}{sep}{s.name}\") for s in f.dataType.fields]\n",
    "                df = df.select(*keep, *exp)\n",
    "                flattened = True\n",
    "                break\n",
    "        if flattened: continue\n",
    "\n",
    "        # explode one array\n",
    "        exploded = False\n",
    "        for f in df.schema.fields:\n",
    "            if isinstance(f.dataType, ArrayType):\n",
    "                df = df.withColumn(f.name, F.explode_outer(F.col(f.name)))\n",
    "                exploded = True\n",
    "                break\n",
    "        if exploded: continue\n",
    "\n",
    "        return df\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def upsert_merge(df, target_table: str, keys: list[str], partition_by: list[str] | None = None):\n",
    "    \"\"\"\n",
    "    Upsert df into target_table on 'keys'. Creates the table if it doesn't exist.\n",
    "    - Keeps all columns from df (update all / insert all)\n",
    "    - Preserves partitioning you choose on the first create\n",
    "    \"\"\"\n",
    "    spark = df.sparkSession\n",
    "    table_exists = spark.catalog.tableExists(target_table)\n",
    "\n",
    "    if not table_exists:\n",
    "        w = df.write.format(\"delta\").mode(\"overwrite\")\n",
    "        if partition_by:\n",
    "            w = w.partitionBy(*partition_by)\n",
    "        w.saveAsTable(target_table)\n",
    "        return\n",
    "\n",
    "    cond = \" AND \".join([f\"t.{k} = s.{k}\" for k in keys])\n",
    "\n",
    "    non_keys = [c for c in df.columns if c not in keys]\n",
    "    set_map = {c: F.col(f\"s.{c}\") for c in non_keys}\n",
    "\n",
    "    print(cond)\n",
    "    print (set_map)\n",
    "\n",
    "    target = DeltaTable.forName(spark, target_table)\n",
    "    (target.alias(\"t\")\n",
    "           .merge(df.alias(\"s\"), cond)\n",
    "           .whenMatchedUpdate(set=set_map)\n",
    "           .whenNotMatchedInsertAll()\n",
    "           .execute())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f45452c-9aab-4dcc-91a3-8c3d5cf76761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from http_connector import build_session, fetch_json\n",
    "#from bronze import write_bronze_raw\n",
    "#from silver import infer_schema_from_sample, parse_raw_to_cols, flatten_structs_until_done, flatten_and_explode_all, upsert_merge\n",
    "#from endpoints import ENDPOINTS\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "BASE_URL = \"https://api.collegefootballdata.com\"\n",
    "\n",
    "def run_ingest_to_bronze(spark, api_key: str, bronze_schema: str, endpoint: str, season: int, week: int):\n",
    "    conf = ENDPOINTS[endpoint]\n",
    "    url_path = conf[\"url\"]({\"season\": season, \"week\": week})\n",
    "    print(url_path)\n",
    "    session = build_session(api_key)\n",
    "    records = fetch_json(session, BASE_URL, url_path)\n",
    "    if not records:\n",
    "        print(f\"[{endpoint}] no rows for season={season} week={week}\")\n",
    "        return f\"{bronze_schema}.{endpoint}_raw\"\n",
    "    return write_bronze_raw(spark, records, conf[\"target\"], bronze_schema, season, week)\n",
    "\n",
    "def run_build_silver_flat(spark, silver_schema: str, bronze_schema: str, endpoint: str, season: int, week: int, mode: str = \"explode_all\"):\n",
    "    conf = ENDPOINTS[endpoint]\n",
    "    raw_tbl = f\"{bronze_schema}.{conf['target']}_raw\"\n",
    "    silver_tbl = f\"{silver_schema}.{conf['target']}_flat_rows\"\n",
    "    schema = infer_schema_from_sample(spark, raw_tbl, season, week)\n",
    "    if not schema:\n",
    "        print(f\"[{endpoint}] no sample to infer schema; skipping\")\n",
    "        return None\n",
    "\n",
    "    df = parse_raw_to_cols(spark, raw_tbl, season, week, schema)\n",
    "\n",
    "    if mode == \"explode_all\":\n",
    "        flat = flatten_and_explode_all(df)\n",
    "    else:\n",
    "        flat = flatten_structs_until_done(df)\n",
    "\n",
    "    # OPTIONAL: clean up column prefixes for common arrays/structs\n",
    "    for old in list(flat.columns):\n",
    "        for p in (\"teams_\", \"stats_\", \"linescores_\", \"plays_\", \"events_\"):\n",
    "            if old.startswith(p):\n",
    "                flat = flat.withColumnRenamed(old, old[len(p):])\n",
    "    '''\n",
    "    target = f\"{silver_schema}.{conf['target']}_{'flat' if mode=='flatten_only' else 'flat_rows'}\"\n",
    "    (flat.write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", True)\n",
    "         .saveAsTable(target))\n",
    "    return target'''\n",
    "\n",
    "    upsert_merge(flat, silver_tbl, keys=[\"id\",\"teamId\",\"stats_category\"], partition_by=[\"_season\",\"_week\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35cc0799-42eb-457c-919e-5cf0a696dad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Callable\n",
    "\n",
    "def url_games(p):   return f\"/games?year={p['season']}&week={p['week']}\"\n",
    "def url_games_teams(p):   return f\"/games/teams?year={p['season']}&week={p['week']}\"\n",
    "def url_games_players(p):return f\"/games/players?year={p['season']}&week={p['week']}\"\n",
    "\n",
    "ENDPOINTS: Dict[str, Dict] = {\n",
    "    \"games\": {\n",
    "        \"url\": url_games,\n",
    "        \"target\": \"games\",             # base table name\n",
    "        \"mode\": \"explode_all\",               # per season/week slice\n",
    "    },\n",
    "    \"games_teams\": {\n",
    "        \"url\": url_games_teams,\n",
    "        \"target\": \"games_teams\",\n",
    "        \"mode\": \"explode_all\",              # per season slice\n",
    "    },\n",
    "    \"games_players\": {\n",
    "        \"url\": url_games_players,\n",
    "        \"target\": \"games_players\",\n",
    "        \"mode\": \"explode_all\",\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334803c3-7d97-47dd-931c-6528e2a9fadd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Databricks notebook\n",
    "dbutils.widgets.text(\"endpoint\", \"games_teams\")\n",
    "dbutils.widgets.text(\"season\", \"2024\")\n",
    "dbutils.widgets.text(\"week\", \"1\")\n",
    "dbutils.widgets.text(\"division\", \"fbs\")\n",
    "dbutils.widgets.text(\"catalog\", \"cfbd\")\n",
    "\n",
    "endpoint = dbutils.widgets.get(\"endpoint\")\n",
    "season   = int(dbutils.widgets.get(\"season\"))\n",
    "week     = int(dbutils.widgets.get(\"week\"))\n",
    "division = dbutils.widgets.get(\"division\")\n",
    "catalog  = dbutils.widgets.get(\"catalog\")\n",
    "\n",
    "bronze_schema = f\"{catalog}.bronze\"\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog} MANAGED LOCATION 's3://databricks-workspace-stack-fed2d-bucket/unity-catalog/455082732875091'\")\n",
    "spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS {bronze_schema}\")\n",
    "\n",
    "#from cfbd.jobs import run_ingest_to_bronze\n",
    "\n",
    "api_key = dbutils.secrets.get(\"cfbd\", \"api-key\")  # scope 'cfbd', key 'api_key'\n",
    "\n",
    "tbl = run_ingest_to_bronze(spark, api_key, bronze_schema, endpoint, season, week)\n",
    "print(f\"Bronze table: {tbl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "734b3c7f-0910-4afb-be46-5f4ae6460d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Databricks notebook\n",
    "dbutils.widgets.text(\"endpoint\", \"games_teams\")\n",
    "dbutils.widgets.text(\"season\", \"2024\")\n",
    "dbutils.widgets.text(\"week\", \"1\")\n",
    "dbutils.widgets.text(\"catalog\", \"cfbd\")\n",
    "dbutils.widgets.text(\"mode\", \"explode_all\")  # \"flatten_only\" or \"explode_all\"\n",
    "\n",
    "endpoint = dbutils.widgets.get(\"endpoint\")\n",
    "season   = int(dbutils.widgets.get(\"season\"))\n",
    "week     = int(dbutils.widgets.get(\"week\"))\n",
    "catalog  = dbutils.widgets.get(\"catalog\")\n",
    "mode     = dbutils.widgets.get(\"mode\")\n",
    "\n",
    "bronze_schema = f\"{catalog}.bronze\"\n",
    "silver_schema = f\"{catalog}.silver\"\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog} MANAGED LOCATION 's3://databricks-workspace-stack-fed2d-bucket/unity-catalog/455082732875091'\")\n",
    "spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS {bronze_schema}\")\n",
    "spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS {silver_schema}\")\n",
    "\n",
    "#from cfbd.jobs import run_build_silver_flat\n",
    "target = run_build_silver_flat(spark, silver_schema, bronze_schema, endpoint, season, week, mode)\n",
    "print(f\"Silver table: {target}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "scratch",
   "widgets": {
    "catalog": {
     "currentValue": "cfbd",
     "nuid": "3c9af8ae-9eb0-4e3f-a5de-9b53b4340a59",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "cfbd",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "cfbd",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "division": {
     "currentValue": "fbs",
     "nuid": "f78a9d43-7d14-4788-8517-1f58b8a6cbcf",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "fbs",
      "label": null,
      "name": "division",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "fbs",
      "label": null,
      "name": "division",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "endpoint": {
     "currentValue": "games_teams",
     "nuid": "f4198d85-0bb0-43cb-be73-15f54a03d644",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "games_teams",
      "label": null,
      "name": "endpoint",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "games_teams",
      "label": null,
      "name": "endpoint",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "mode": {
     "currentValue": "explode_all",
     "nuid": "82047b68-6afc-40c7-90c2-50a876265768",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "explode_all",
      "label": null,
      "name": "mode",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "explode_all",
      "label": null,
      "name": "mode",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "season": {
     "currentValue": "2024",
     "nuid": "fe73213f-f9e4-421d-b3a5-c50551b48937",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2024",
      "label": null,
      "name": "season",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2024",
      "label": null,
      "name": "season",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "week": {
     "currentValue": "1",
     "nuid": "7abc0f11-77b3-4c09-9598-62e876441272",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1",
      "label": null,
      "name": "week",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "1",
      "label": null,
      "name": "week",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
